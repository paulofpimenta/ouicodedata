<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>
    
      Jekyll blog
    
  </title>

  <!-- Jquery -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>

  <!--Flickity -->
  <script src="https://unpkg.com/flickity@2/dist/flickity.pkgd.min.js"></script>
  
  <!--Bootstrap-->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js" integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf" crossorigin="anonymous"></script>
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css">

  <!-- Fontsawesome -->
  <link rel="stylesheet" href="/assets/font-awesome/css/all.css">
  <link rel="stylesheet" href="/assets/font-awesome/css/v4-shims.css">

  <!-- Social icons -->
  <link rel="stylesheet" href="/static/css/social-icons.css">
  

  <meta name="description" content="A website about spatial data, artificial intelligence and data science Google search results) and in your feed.xml site description.">

  <link rel="canonical" href="https://paulofpimenta.github.io/ouicodedata//page2/">
  <link rel="alternate" type="application/rss+xml" title="OuiCodeData" href="https://paulofpimenta.github.io/ouicodedata//feed.xml">

  <link href='https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|Roboto+Condensed:700&subset=latin' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/assets/main.css">

  <meta property="og:url" content="https://paulofpimenta.github.io/ouicodedata//page2/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Paulo Pimenta, Phd">
  <meta property="og:description" content="Data Scientist">
  <meta property="og:site_name" content="OuiCodeData">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="https://paulofpimenta.github.io/ouicodedata//page2/">
  <meta name="twitter:title" content="Paulo Pimenta, Phd">
  <meta name="twitter:description" content="Data Scientist">

  
    <meta property="og:image" content="https://paulofpimenta.github.io/ouicodedata//assets/images/rails.jpg">
    <meta name="twitter:image" content="https://paulofpimenta.github.io/ouicodedata//assets/images/rails.jpg">
  


</head>


  <body>
    <div id="shadow"></div>

      <header class="main-header content-wrapper">

  <input type="checkbox" id="menu-checkbox" />

  <nav class="center-wrapper nav-main">

    <a class="blog-logo" href="/">OuiCodeData</a>

    
      
        <a href="/about/">About</a>
      
    
      
    
      
        <a href="/posts/">Blog</a>
      
    
      
        <a href="/books/">Favorite books</a>
      
    
      
    
      
    
      
    
      
    
      
        <a href="/portfolio/">Portfolio</a>
      
    
      
        <a href="/resume/">Resume</a>
      
    
      
    
      
    
      
    


    <label for="menu-checkbox" class="toggle-button" data-open="☰" data-close="☰" onclick></label>
  </nav>

</header>

      
  <aside class="sidebar" role="note" style="background-image: url(/assets/images/rails.jpg)">  


  <div class="cover">

    <div class="cover-text">
      <div class="heading">

        
          Paulo Pimenta, Phd
        
      </div>

      <p>
        
          Data Scientist
        
        
      </p>

    </div>

  </div>

  <div id="switcher"></div>
</aside>


    <main class="content-wrapper blog-content">
        <!--Left content -->
  <div class="home col-md-12" >
    <!--Posts-->
    
    <div class="post-contents-shadow">
      <h2>
        <a class="post-link" href="/posts/worldclim-with-r/">Worldclim data with R: the right way</a>
      </h2>
      <span class="post-meta" style="margin-right:50px">
          <i class="fas fa-user">&nbsp;&nbsp;</i>By Paulo Pimenta
      </span>
      
      <span class="post-meta"><i class="fa fa-calendar">&nbsp;&nbsp;</i>Oct 13, 2023</span>
      <span class="post-meta" style="margin-left:50px;margin-right:50px;">
        <i class="fa fa-comment-o">&nbsp;&nbsp;</i>
        <a style="color:#828282" href="/posts/worldclim-with-r/#disqus_thread"></a>
      </span>

      
        <span class="post-meta" ><i class="fa fa-tags">&nbsp;&nbsp;</i>Spatial data</span>
      

      <div class="post-content truncate" itemprop="articleBody" style="max-height:300px;margin-top:20px;text-align: justify; margin-bottom: 10px;">
	      <h3 id="introduction">Introduction</h3>

<p>Worldclim data is a great source of environmental data opensource data. Its a database of high spatial resolution global weather and climate data. Worldclim <a class="citation" href="#worldclim">(Fick and Hijmans, 2017)</a> is in its version 2.1 and it has climate data from 1970 to 2000. Containing several bioclimatic variables derived from monthly temperatre and rainfall values, the data is usually applied to ecology and modelling techniques. Although you can download all the data <a href="https://worldclim.org/data/index.html">here</a>, Worldclim data can also be easily downloaded using the  <code class="language-plaintext highlighter-rouge">raster</code> package with <code class="language-plaintext highlighter-rouge">R</code> using few lines of code.</p>

<p>This approach will, however , soon be deprecated due to the <a href="https://r-spatial.org/r/2022/04/12/evolution.html">retirement of rgdal, rgeos and maptools</a>. Hopefully, Worldclim is still easlly acessible. For this post, we will focus on how to extract and plot data from Worldclim, focusing on preciptation raster in Nigeria</p>

<h3 id="the-right-way">The right way</h3>

<p>Any user with litte experience with R may quickly acess the worldclim large set of raster files containing many environmental data. With the deprecation <code class="language-plaintext highlighter-rouge">rdal</code> and others, wordclim is managed by the <a href="https://cran.r-project.org/web/packages/geodata/geodata.pdf">geodata</a> package. Depending on your R versiom, you might still be able to run <code class="language-plaintext highlighter-rouge">raster</code> and <code class="language-plaintext highlighter-rouge">rgal</code> packages, but its very likelly will you find some warnings. Plus, it is better to get used to the new practices of spatial data manipulation in <code class="language-plaintext highlighter-rouge">R</code>, that is, the use of the <code class="language-plaintext highlighter-rouge">sf</code> package. Let’s start !</p>

<h3 id="set-raster-and-vector-path">Set raster and vector path</h3>

<p>We start by defining raster and vector paths. For raster data, you can set <code class="language-plaintext highlighter-rouge">raster_path = tempdir()</code> if you want to download the data into a temp folder. If not, set it to folder of preference. You can also do the same for <code class="language-plaintext highlighter-rouge">vector_output_path</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raster_path_folder</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tempdir</span><span class="p">()</span><span class="w"> 
</span><span class="n">vector_output_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"~/nigeria_adm2"</span><span class="w">
</span><span class="n">vector_output_file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="n">vector_output_path</span><span class="p">,</span><span class="w"> </span><span class="s2">".zip"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h3 id="load-data-from-worldclim">Load data from worldclim</h3>

<p>Now, we use a worlclim function called <code class="language-plaintext highlighter-rouge">worldclim_country</code>. The <code class="language-plaintext highlighter-rouge">res</code> paramater is the resolution, with valid values as 10,5,2.5 and 0.5. The <code class="language-plaintext highlighter-rouge">var</code> parameter is the variable name, with valid values as <code class="language-plaintext highlighter-rouge">tmin</code>, <code class="language-plaintext highlighter-rouge">tmax</code>, <code class="language-plaintext highlighter-rouge">tavg</code>, <code class="language-plaintext highlighter-rouge">prec</code>, <code class="language-plaintext highlighter-rouge">wind</code>,
<code class="language-plaintext highlighter-rouge">vapr</code> and <code class="language-plaintext highlighter-rouge">bio</code>.</p>

<p>Using the code bellow, we chose <code class="language-plaintext highlighter-rouge">prec</code> since we are interested in precipitation data. Thus, the <code class="language-plaintext highlighter-rouge">worldclim_country</code> function will return 12 layers of raster (in a raster stack), each image representing a month of precipitation in Nigeria (NGA). We later save the coordinate system of the data for later use in line 2</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raster_stack</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">worldclim_country</span><span class="p">(</span><span class="n">country</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"NGA"</span><span class="p">,</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">raster_path_folder</span><span class="w"> </span><span class="p">,</span><span class="n">version</span><span class="o">=</span><span class="s2">"2.1"</span><span class="p">,</span><span class="n">res</span><span class="o">=</span><span class="m">0.5</span><span class="p">,</span><span class="n">var</span><span class="o">=</span><span class="s2">"prec"</span><span class="p">)</span><span class="w">
</span><span class="n">crs_proj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">crs</span><span class="p">(</span><span class="n">raster_stack</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h3 id="download-the-shapefile">Download the shapefile</h3>

<p>You can manually download and extract data from the OCHC site. The nigeria administrative level 2 vector data is avaiable <a href="https://data.humdata.org/dataset/nigeria-admin-level-2">here</a>. Or, you can just run these two lines :</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">download.file</span><span class="p">(</span><span class="s2">"https://data.humdata.org/dataset/aac2a1d6-36c6-4f47-beee-34415742180d/resource/d7011402-0a22-4927-82eb-1359d17ff5cd/download/nigeria_admin_level_2.zip"</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">destfile</span><span class="o">=</span><span class="n">vector_output_file</span><span class="p">)</span><span class="w">
</span><span class="n">unzip</span><span class="p">(</span><span class="n">vector_output_file</span><span class="p">,</span><span class="w"> </span><span class="n">exdir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vector_output_path</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h3 id="read-the-downloaded-shape-file">Read the downloaded shape file</h3>

<p>If we want to plot them together, we must be sure that they belong to the same coordinate system. Let’s apply the coordinate systems saved previously in <code class="language-plaintext highlighter-rouge">crs_proj</code> and apply it to the nigeria admin level boundaries</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nigeria_vector_adm2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read_sf</span><span class="p">(</span><span class="n">paste0</span><span class="p">(</span><span class="n">vector_output_path</span><span class="p">,</span><span class="s2">"/"</span><span class="p">,</span><span class="s2">"Nigeria_Admin_Level_2.shp"</span><span class="p">))</span><span class="w">
</span><span class="n">nigeria_vector_adm2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">st_transform</span><span class="p">(</span><span class="n">nigeria_vector_adm2</span><span class="p">,</span><span class="w"> </span><span class="n">st_crs</span><span class="p">(</span><span class="n">raster_stack</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>All we have to do now is to set the plot to 12 subplots, using the <code class="language-plaintext highlighter-rouge">par</code> function, loop over the raster stack and plot nigeria boundaries.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Set up plot area</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">4</span><span class="p">))</span><span class="w">

</span><span class="c1"># Loop layers and plot</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="nf">length</span><span class="p">(</span><span class="n">raster_stack</span><span class="p">[</span><span class="m">1</span><span class="p">])){</span><span class="w">
  </span><span class="n">plot</span><span class="p">(</span><span class="n">raster_stack</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span><span class="w"> </span><span class="n">main</span><span class="o">=</span><span class="nf">names</span><span class="p">(</span><span class="n">raster_stack</span><span class="p">[[</span><span class="n">i</span><span class="p">]]))</span><span class="w">
  </span><span class="n">plot</span><span class="p">(</span><span class="n">st_geometry</span><span class="p">(</span><span class="n">nigeria_vector_adm2</span><span class="p">),</span><span class="n">bg</span><span class="o">=</span><span class="s1">'transparent'</span><span class="p">,</span><span class="w"> </span><span class="n">border</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lightblue"</span><span class="p">,</span><span class="w"> </span><span class="n">add</span><span class="o">=</span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>
<p>And the result :</p>

<p align="center">
  <img src="/assets/images/posts/world_clim/worldclim.png" />
</p>

<h3 id="references">References</h3>

<ol class="bibliography"><li><p><span id="worldclim">Fick, S.E., Hijmans, R.J., 2017. WorldClim 2: new 1-km spatial resolution climate surfaces for global land areas. International Journal of Climatology 37, 4302–4315. https://doi.org/https://doi.org/10.1002/joc.5086</span></p></li></ol>


      </div>
      <div style="height:50px;">
	      <a href="/posts/worldclim-with-r/" class="button readmore-button">Read More</a>
      </div>
    </div>
    <br />
    
    <div class="post-contents-shadow">
      <h2>
        <a class="post-link" href="/posts/gender-detector-with-pytorch/">Gender detection with Pytorch</a>
      </h2>
      <span class="post-meta" style="margin-right:50px">
          <i class="fas fa-user">&nbsp;&nbsp;</i>By 
      </span>
      
      <span class="post-meta"><i class="fa fa-calendar">&nbsp;&nbsp;</i>Jun 12, 2023</span>
      <span class="post-meta" style="margin-left:50px;margin-right:50px;">
        <i class="fa fa-comment-o">&nbsp;&nbsp;</i>
        <a style="color:#828282" href="/posts/gender-detector-with-pytorch/#disqus_thread"></a>
      </span>

      
        <span class="post-meta" ><i class="fa fa-tags">&nbsp;&nbsp;</i>Deep Learning</span>
      

      <div class="post-content truncate" itemprop="articleBody" style="max-height:300px;margin-top:20px;text-align: justify; margin-bottom: 10px;">
	      <p>Last month I spent quite some time diving into the wonderful world of Pytorch library for deep learning. My objective is to post a series of applications with pytorch features. But for now I will focus on how to build an gender detection application with Pytorch using the webcam as input</p>

<h3 id="get-the-dataset-first-but-which-one-and-why-">Get the dataset first. But which one and why ?</h3>

<p>It is not difficult to find some available datasets containing human faces. The sources are numerous : The <a href="https://susanqq.github.io/UTKFace/">UTKFace</a>; the famous <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a> database with more than 200 thousand faces of celebrities; <a href="https://github.com/microsoft/DigiFace1M">DigiFace</a> with its 1.2 million free from privacy violations images, and so on. A good list can be found <a href="https://datagen.tech/blog/face-datasets/">here</a>. In this post, I decided to work with a dataset this dataset <a href="https://www.kaggle.com/datasets/ashwingupta3012/male-and-female-faces-dataset/data">here</a> for three reasons. The first is that this dataset, as many others, has  with a public domain license, but is also very used in kaggle notebooks. The second reason, is how the data is presented and organized.  The other datasets have either spreadsheets with annotations containing name files and the classes thar each file belong to, or, in some cases, classes are not divided by folders. Since my object is to load data from custom dataset (next section :-)), I wanted a dataset that was more structured into folders and classes. And the final reason is the image quality : this dataset does not contain high quality images. Which is is quite well, considering that our input images will come from web cam, that usually have low resolution images. Thus, in this case, a model trained with nice and artistic images may not reflect the context of the application.</p>

<h3 id="how-to-work-with-data-with-pytorch">How to work with data with Pytorch</h3>

<p>Pytorch library is a very rich framework with libraries that fit your need according to the type of problem you want to solve. For instance,<code class="language-plaintext highlighter-rouge">Torchaudio</code> for audio recognition, <code class="language-plaintext highlighter-rouge">Torchvision</code> for images, <code class="language-plaintext highlighter-rouge">Torchtext</code>for NLP, <code class="language-plaintext highlighter-rouge">Torchgeo</code> for satellite images, and so on. Since our problem is image classification, we will use the <code class="language-plaintext highlighter-rouge">Torchvision</code> library. This library has has a very straight forward (but flexible) process about reading data: your first (1) create your dataset, than you transform (2) the data, augment (3) it if necessary, and finally, add it to a dataloader (4) before training. This process can be illustrated by the image bellow:</p>

<p align="center">
  <img src="/assets/images/posts/gender_detect/basic_dataloader_pipeline.png" />
  Source https://arcwiki.rs.gsu.edu
</p>

<p>Before we talk about how to load data with Pytorch, I will briefly present the application and the neural network architecture used in this work</p>

<h3 id="application-architecture">Application architecture</h3>

<p>As stated before, the idea is to have a web application that takes live screenshots from the user’s webcam performs live predictions as a result. The application was built according to the following schema :</p>

<p align="center">
  <img src="/assets/images/posts/gender_detect/app1_architecture.png" width="500" />
  <p align="center"><strong>Application main architecture</strong></p>
</p>

<p>In this post, we will focus only on the Pytorch side, which is the aim of this post. However, it is import to keep in mind that models applied to real cases might suffer from some sort of technical limitation. These limitations include, but not limited to, poor infrastructure resources, nature of training data, model complexity or even, deployment issues. We will come to that later</p>

<h3 id="neural-network-architecture">Neural network architecture</h3>

<p>The architecture of a neural network is core of deep learning application, since it can provide the most optimized way to deal if the data thought the many layers of input neurons. Our architecture is the same used in <a class="citation" href="#Levi2015AgeAG">(Levi and Hassner, 2015)</a></p>

<h3 id="building-your-custom-set">Building your custom set</h3>

<p><code class="language-plaintext highlighter-rouge">Torchvision</code> is a package in the PyTorch library containing computer-vision models, datasets, and image transformations. A famous dataset in the domain of Machine Learning (the MNIST) dataset, could be easily load like this :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="n">datasets</span>

<span class="c1"># Load train set
</span><span class="n">mnist_trainset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c1"># Load test set
</span><span class="n">mnist_testset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p>Notice that <code class="language-plaintext highlighter-rouge">datasets.MNIST</code> is already provided out of the box to us. But in real data problems, that is usually not the case. Thus we will have to create our own datataset. But, as I said, Pytorch makes it quite easy :-)</p>

<p>In order to create your own data set, you must create a class that inhertis the <code class="language-plaintext highlighter-rouge">torch.utils.data.Dataset</code> abstract class. Our custom dataset class must also override the methods <code class="language-plaintext highlighter-rouge">__len__</code>  and <code class="language-plaintext highlighter-rouge">__getitem__</code>. They are respectively responsible for returning the size of the dataset and provide support for the indexing samples.</p>

<p>Let’s create our custom dataset called GenderDataSet as follows :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GenderDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_paths</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span><span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">image_paths</span> <span class="o">=</span> <span class="n">image_paths</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="c1"># Select images by path and indexes
</span>        <span class="n">image_filepath</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">read_image</span><span class="p">(</span><span class="n">image_filepath</span><span class="p">)</span>
        <span class="c1"># Create dictionary for class indexes
</span>        <span class="n">idx_to_class</span> <span class="o">=</span>  <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">j</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">classes</span><span class="p">)}</span>
        <span class="n">class_to_idx</span> <span class="o">=</span>  <span class="p">{</span><span class="n">value</span><span class="p">:</span><span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span><span class="n">value</span> <span class="ow">in</span> <span class="n">idx_to_class</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="c1"># Replace split char by '/' on unix systems
</span>        <span class="n">label</span> <span class="o">=</span> <span class="n">image_filepath</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\\</span><span class="s">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">class_to_idx</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
        <span class="c1"># Appy transform if there any is provided
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">transform</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="c1"># Return image and its label
</span>        <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
</code></pre></div></div>
<p>When we instantiate our dataset, we added 3 more class attributes : <code class="language-plaintext highlighter-rouge">image_paths</code>, <code class="language-plaintext highlighter-rouge">classes</code> and <code class="language-plaintext highlighter-rouge">transform</code>. Remember about why I chose this dataset ? Since the dataset contains folders per class, and each class has a <code class="language-plaintext highlighter-rouge">train</code> and <code class="language-plaintext highlighter-rouge">test</code> folder, all we have to do is to point where the train and test folders are. That implementation will take care of of rest, for each class present in the root folder :-)</p>

<p>So we instantiate the model <a class="citation" href="#ruby">(Flanagan and Matsumoto, 2008)</a>.</p>

<p>#  Instantiating model
    conv_net = ConvNet()
    model = ConvolutionalNeuralNet(conv_net)
    print(conv_net)</p>

<h4 id="references">References</h4>

<ol class="bibliography"><li><p><span id="ruby">Flanagan, D., Matsumoto, Y., 2008. The Ruby Programming Language. O’Reilly Media.</span></p></li>
<li><p><span id="Levi2015AgeAG">Levi, G., Hassner, T., 2015. Age and gender classification using convolutional neural networks. 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) 34–42.</span></p></li></ol>


      </div>
      <div style="height:50px;">
	      <a href="/posts/gender-detector-with-pytorch/" class="button readmore-button">Read More</a>
      </div>
    </div>
    <br />
    

    <!-- Paginator-->
    <!-- pagination -->
    <nav aria-label="Page navigation example">

    
    <ul class="pagination justify-content-center">
      
        <li class="page-item ">
          <a href="/" tabindex="-1" >&laquo; Prev</a>
        </li>

      

      
        
          <li class="page-item">
            <a href="/" >1</a>
          </li>
        
      
        
          <li class="page-item">
            <span>2</span>
          </li>
        
      

      
        <li class="page-item">
          <span>Next &raquo;</span>
        <li>
      
    </ul>
    
    </nav>
  <!-- Left content-->
  </div>
    </main>

    <footer class="blog-footer content-wrapper">
  <p>&copy; <span class="full-year"></span> OuiCodeData</p>
</footer>
<script src="/assets/js/scripts.js"></script>


  </body>
</html>
